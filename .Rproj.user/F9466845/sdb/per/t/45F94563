{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Checklist_for_Data_Cleansing\"\nauthor: \"Sebastian Sauer\"\ndate: \"9 August 2016\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, cache = TRUE)\n```\n\n\n# What this post is about: Data cleansing in practice with R\n\nData analysis, in practice, consists typically of some different steps which can be subsumed as \"preparing data\" and \"model data\" (not considering communication here):\n\n![](data_science_process.png)\n\n(Inspired by [this](http://r4ds.had.co.nz/introduction-1.html))\n\nOften, the first major part -- \"prepare\" -- is the most time consuming. This can be lamented since many analysts prefer the cool modeling aspects (since I want to show my math!). In practice, one rather has to get his (her) hands dirt...\n\nIn this post, I want to put together some kind of checklist of frequent steps in data preparation. More precisely, I would like to detail some typical steps in \"cleansing\" your data. Such steps include: \n\n___\n\n- [x] identify missings \n- [x] identify outliers \n- [x] check for overall plausibility and errors (e.g, typos)\n- [x] identify highly correlated variables\n- [x] identify variables with (nearly) no variance\n- [x] identify variables with strange names or values\n- [x] check variable classes (eg. characters vs factors)\n- [x] remove/transform some variables (maybe your model does not like categorial variables)\n- [x] rename some variables or values (especially interesting if large number)\n- [x] check some overall pattern (statistical/ numerical summaries)\n- [x] center/scale variables\n\n___\n\n\n# Don't get lost in big projects\nBefore we get in some details, let's consider some overall guidelines. I have noticed that some projects keep growing like weed, and I find myself bewildered in some jungle... The difficulties then arise not because data or models are difficult, but due to the sheer volume of the analysis.\n\n## All in a function\nPut analytical steps which belong together in one function. For example, build one function for data cleansing, give as input the raw data frame and let it spit out the processed data frame after all your cleansing steps:\n\n```{r eval = FALSE}\n\ncleansed_data <- cleanse_data(raw_dirty_data,\n                              step_01 = TRUE,\n                              step_02 = TRUE,\n                              step_03 = TRUE)\n\n```\n\nAlthough functions are a bit more difficult to debug, at the end of the day it is much easier. Normally or often the steps will be run many times (for different reasons), so it is much easier if all is under one roof.\n\nThat said, [pragmtic programming](http://blog.sukria.net/2012/01/06/the-10-rules-of-the-pragmatic-programmer/) suggests to start easy, and to refactor frequently. So better start with a simple solution that works than to have a enormous code that chokes. Get the code running, then improve on it.\n\n# Data set for practice\n\nThe **OKCupid** Data set (sanitized version, no names!) is quite nice. You can download it [here](https://github.com/rudeboybert/JSE_OkCupid).\n\nIn the following, I will assume that these data are loaded.\n\n```{r}\nlibrary(readr)\n\npath <- \"/Users/sebastiansauer/Documents/OneDrive/Literatur/Methoden_Literatur/Datensaetze/JSE_OkCupid/\"\n\nfile <- \"profiles.csv\"\n\ndata <- read_csv(paste(path, file, sep = \"\"))\n\n```\n\nSo, the data set is quite huge: `r dim(data)` (rows/cols)\n\nLet's have a brief look at the data.\n\n```{r}\nlibrary(dplyr)\nglimpse(data)\n```\n\n\n## What's in a name?\nWith the code getting longer, it is easy to get confused about naming: `data_v2_no_missings_collapsed` is the right data matrix to proceed, wasn't it? Or rather `data_dat_edit_noNA_v3`? Probably a helpful (though partial) solution is to prevent typing a lot. \"Don't repeat yourself\" - if stuff is put inside a function, the objects will not clutter your environment, and you don't have to deal with them all the time. Also, you will reduce code, and the number of objects if stuff is put inside functions and loops.\n\nThat raises the question who to name the data set? At least two points are worth thinking. First, the \"root\" name should it be the name of the project such as \"OKCupid\" or \"nycflights13\"? Or just something like \"data\"? Personally, I prefer \"data\" as it is short and everybody will know what's going on. Second question: Should some data manipulations should be visible in the name of the object? I, personally, like this and do that frequently, eg., `carat_mean`, so I will remember that the mean is in there. However, for bigger projects I have made the experience that I lose track on which is the most recent version. So better do *not* put data manipulations in the name of the data frame. However, what one can do is use attributes, eg.:\n\n```{r eval = FALSE}\n\ndata_backup <- removing_missings(data)\ndata <- data_backup\n\nattr(data, \"NA_status\") <- \"no_NA\"\n\n\n```\n\n\n# Checklist\n\nClearly, there are different ways to get to Rome; I present just one, which has proved helpful for me.\n\n## Identify missings\nA key point is here to address many columns in one go, otherwise it gets laborious with large data sets. Here's one way:\n\n```{r identify_missings}\n\nlibrary(knitr)\n\ndata %>% \n  summarise_all(funs(sum(is.na(.)))) %>% kable\n```\n\nThe function `kable` prints a html table (package `knitr`).\n\n\nThere seem to be quite a bit missings. Maybe better plot it.\n\n```{r plot_missings}\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndata %>% \n  summarise_all(funs(sum(is.na(.)))) %>% \n  gather %>% \n  ggplot(aes(x = reorder(key, value), y = value)) + geom_bar(stat = \"identity\") +\n  coord_flip() +\n  xlab(\"variable\") +\n  ylab(\"Absolute number of missings\")\n\n```\n\nWith this large number of missings, we probably will not find an easy solution. Skipping cases will hurt, but imputating may also not be appropriate. Ah, now I know: Let's just leave it as it is for the moment :-)\n\nOr, at least let's remember which columns have more than, say, 10%, missings:\n\n```{r}\n\ncols_with_some_NA <- round(colMeans(is.na(data)),2)\ncols_with_too_many_NA <- cols_with_some_NA[cols_with_some_NA > .1]\n\n# alterntively:\ndata %>% \n  select_if(function(col) mean(is.na(col)) < .1)\n\n```\n\nOK, that are `length(cols_with_too_many_NA)` columns. We would not want to exclude them because they are too many.\n\n\n\n## Identify outliers\nObviously, that's a story for numeric variable only. So let's have a look at them first.\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% names\n```\n\nHistograms are a natural and easy way to spot them and to learn something about the distribution.\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  gather %>% \n  ggplot(aes(x = value)) + facet_wrap(~ key, scales = \"free\", nrow = 3) +\n  geom_histogram()\n```\n\nEspecially `income` may be problematic. \n\nFind out more on `gather` eg., [here](https://sebastiansauerblog.wordpress.com/2016/07/13/long-vs-wide-format-and-gather/).\n\nBox plots (or violin plots/bean plots) may also be a way:\n\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  gather %>% \n  ggplot(aes(x = 1, y = value)) + facet_wrap(~ key, scales = \"free\") + \n  geom_violin() +\n  ylab(\"Value\") +\n  xlab(\"Variable\")\n```\n\n\n## Identify variables with unique values\n\nSimilar to outliers, for categorical variable we can look whether some values are seldom, e.g, only 0.1% of the times. What \"often\" or \"barely\" is, depends on ... you!\n\n\n\n```{r fig.height = 15}\nlibrary(purrr)\n\ndata %>% \n  select_if(negate(is.numeric)) %>% \n  select(-matches(\"essay\")) %>% \n  select(-last_online) %>% \n  gather %>% \n  ggplot(aes(x = value)) + geom_bar() + \n  facet_wrap(~ key, scales = \"free\", ncol = 3) \n```\n\n\nPooh, that takes ages to plot. And does not look too pretty. Maybe better don't plot, since we are not after exploration, but just want to know if something is going wrong.\n\n\nMaybe it is better if we do the following:\n> for each non-numeric variable do\n>     divide most frequent category by least frequent category\n\nThis gives an indication whether some categories are quite frequent in relation to others.\n\n\n```{r}\n\n\ndata %>% \n  select_if(is.character) %>% \n  summarise_each(funs(max(table(.)/(min(table(.)))))) %>% \n  arrange %>% \n  kable\n  \n```\n\n\n## Plausibility check\n\nPlausibility check can includes checking orders of magnitude, looking for implausible values (negative body weight), among others. A good starter is to differentiate between numeric and non-numeric variables.\n\n\n### Numeric\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  map(summary)\n```\n\nThe function `map` comes from package `purrr`; it maps each selected column to the function `summary`.\n\nFor instance, the max. age of 110 appears somewhat high... And the min. height of 1 should rather be excluded before further operations start. For `income` similar reasoning applies.\n\n### Non-numeric\n\nLet's do not look at all these `essay` variables here, because \"strange\" values are not so straight forward to identify compared to more normal categorical variables (with less distinct values).\n\n```{r, eval = FALSE}\ndata %>% \n  select(-matches(\"essay\")) %>% \n  select_if(is.character) %>% \n  mutate_all(factor) %>% \n  map(summary)\n\n```\n\n*Output truncated, TL;DR*\n\nWe need to convert to `factor` because for `character` variables, no nice `summary` is available of the shelf.\n\n\n## Highly correlated variables\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  cor\n```\n\n\n## Constants/ near zero variance variables\n\nApplies to numeric variables obviously.\n\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  na.omit %>% \n  summarise_all(c(\"sd\", \"IQR\"))\n```\n\n\n\n## Rename many variables\n\n```{r}\nncol_data <- ncol(data) \nnames_data_new <- paste(\"V\",1:ncol_data, sep = \"\")\ndummy <- data\nnames(dummy) <- names_data_new\n```\n\n\n## Recode values\n\nA quite frequent use case is to get \"strange\" value or variables names in order, e.g., \"variable 2\" or \"I rather not say\" (including blanks or some other non-normal stuff).\n\nOne approach is to use `dplyr::recode`.\n```{r}\ndplyr::distinct(data, body_type)\n\ndummy <- dplyr::recode(data$body_type, `a little extra` = \"1\")\nunique(dummy)\n\n```\n\n\nA second approach is to use `base::levels` for factors.\n\n```{r}\ndummy <- data$body_type \ndummy <- factor(dummy)\nlevels(dummy)\nlevels(dummy) <- 1:12\nlevels(dummy)\n```\n\nI don not use `plyr:mapvalues` because `plyr` can interfere with `dplyr` (and `dplyr` seems more helpful to me).\n\n## Center/ scale variables\n\nAs often, several approaches. One is:\n\n```{r}\ndata %>% \n  select_if(is.numeric) %>% \n  scale() %>% \n  head\n```\n\n`base::scale` performs a z-transformation of the matrix (like object) given as input.\n\nBut wait; one issue is that the data frame now consists of the numeric variables only. We have to manually knit it together with the rest of the party. Not so convenient. Rather, try this:\n\n```{r}\ndata %>% \n  select(-matches(\"essay\")) %>% \n  mutate_if(is.numeric, scale) %>% \n  glimpse\n```\n\nIf we only want to center (or something similar), we could do\n\n```{r}\ndata %>% \n  mutate_if(is.numeric, funs(. - mean(.)))\n```\n\n\nThat's it; happy analyzing!\n",
    "created" : 1470741126942.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2594012118",
    "id" : "45F94563",
    "lastKnownWriteTime" : 1470814897,
    "last_content_update" : 1470814897188,
    "path" : "~/Documents/OneDrive/Forschung/blog/posts/checklist_data_cleansing/Post_data_cleansing.Rmd",
    "project_path" : "Post_data_cleansing.Rmd",
    "properties" : {
        "chunk_rendered_width" : "650",
        "ignored_words" : "OKCupid,nycflights\n",
        "tempName" : "Untitled2"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}